# SynSeekr API Reference

> **When to read this:** Writing code, adding endpoints, understanding main.py patterns, calling external service APIs.

---

## Table of Contents

### Internal APIs (Orchestrator)
1. [Code Organization](#1-code-organization)
2. [Helper Functions](#2-helper-functions)
3. [Data Models](#3-data-models)
4. [Key Endpoints](#4-key-endpoints)
5. [Code Patterns](#5-code-patterns)
6. [Adding New Features](#6-adding-new-features)

### External Service APIs
7. [Authentik SSO API](#7-authentik-sso-api)
8. [Unifi Network API](#8-unifi-network-api)
9. [Grafana API](#9-grafana-api)

---

## 1. Code Organization

### Main File Structure

`/docker/orchestrator/main.py` (~11,000 lines) is organized as:

```
Lines 1-200      Imports, configuration, globals
Lines 200-1200   Embedded HTML/JS UI (the /ui page)
Lines 1200-2000  Model loading, startup tasks
Lines 2000-3500  Pydantic models (request/response schemas)
Lines 3500-6000  Helper functions
Lines 6000-10000 API endpoints
Lines 10000+     Startup hooks, background workers
```

### Supporting Modules

| File | Purpose | Lines |
|------|---------|-------|
| `main.py` | Core API (monolithic) | ~11,500 |
| `entity_extractor.py` | GLiNER/GLiREL logic | ~1,000 |
| `graph_utils.py` | Neo4j query helpers | ~500 |
| `db/__init__.py` | Database module exports | ~50 |
| `db/analysis.py` | Analysis results SQLite schema | ~250 |
| `db/memory.py` | Conversation memory SQLite schema | ~400 |
| `memory/__init__.py` | Memory module exports | ~30 |
| `memory/retriever.py` | Qdrant conversation_memory search | ~200 |
| `memory/context.py` | Memory context injection helpers | ~250 |
| `memory/api.py` | Memory REST API router | ~430 |
| `task_queue/__init__.py` | Task queue exports | ~20 |
| `task_queue/worker.py` | Dramatiq background workers | ~200 |
| `analysis/__init__.py` | Analysis module exports | ~30 |
| `analysis/classifier.py` | Document type classification | ~100 |
| `analysis/summarizer.py` | LLM document summaries | ~80 |
| `analysis/fact_extractor.py` | Date/amount/party + timeline extraction | ~300 |
| `analysis/hot_scorer.py` | Importance scoring (0-100) | ~120 |
| `analysis/privilege_detector.py` | Attorney-client detection | ~100 |
| `analysis/inconsistency_detector.py` | Timeline conflict detection | ~200 |
| `analysis/api.py` | Analysis REST API router | ~200 |

### Key Globals

```python
# Clients
qdrant_client = QdrantClient(host="qdrant", port=6333)
neo4j_driver = GraphDatabase.driver("bolt://neo4j:7687", auth=("neo4j", "legal-ai-graph-2024"))

# Models (lazy-loaded)
embedding_model = None      # Loaded at startup, stays loaded
whisper_model = None        # Lazy on first audio
cross_encoder = None        # Lazy on first rerank
docling_converter = None    # Lazy on first complex PDF
gliner_model = None         # Lazy on first extraction
glirel_model = None         # Lazy on first extraction

# Queues
entity_extraction_queue = asyncio.Queue()
audio_transcription_queue = asyncio.Queue()
docling_processing_queue = asyncio.Queue()

# Configuration
OLLAMA_URL = "http://ollama:11434"
COLLECTION_NAME = "legal_documents"
EMBEDDING_DIMENSIONS = 1024
```

---

## 2. Helper Functions

### PII Detection & Anonymization

```python
async def detect_pii(text: str) -> List[dict]:
    """
    Detect PII entities in text using Presidio.

    Returns: [{"entity_type": "PERSON", "start": 0, "end": 10, "score": 0.95}, ...]
    """

async def anonymize_text(text: str) -> Tuple[str, dict]:
    """
    Replace PII with tokens like <PERSON_1>, <SSN_1>.

    Returns: (anonymized_text, token_map)
    Token map: {"<PERSON_1>": "John Smith", "<SSN_1>": "123-45-6789"}
    """

def restore_pii(text: str, token_map: dict) -> str:
    """
    Replace tokens back with original values.

    Example: "<PERSON_1> filed suit" → "John Smith filed suit"
    """
```

### LLM Inference

```python
async def query_ollama(
    prompt: str,
    model: str = "qwen2.5:7b",
    system_prompt: str = None,
    temperature: float = 0.7,
    max_tokens: int = 2000
) -> str:
    """
    Query Ollama LLM.

    Models available:
    - "qwen2.5:7b" - Complex analysis, RAG
    - "llama3.2:3b" - JSON structured output

    Returns: Generated text response
    """

async def query_ollama_json(
    prompt: str,
    model: str = "llama3.2:3b"
) -> dict:
    """
    Query Ollama expecting JSON response.
    Automatically parses and validates JSON.

    Returns: Parsed JSON dict
    """
```

### Embeddings

```python
async def get_embedding(text: str) -> List[float]:
    """
    Get 1024-dimensional embedding for text.
    Uses legal-bge-m3 model (always loaded).

    Returns: List of 1024 floats
    """

async def get_embeddings_batch(texts: List[str]) -> List[List[float]]:
    """
    Batch embedding for multiple texts.
    More efficient than calling get_embedding() in loop.

    Returns: List of embedding vectors
    """
```

### Document Search

```python
async def search_documents(
    query: str,
    case_id: str = None,
    client_id: str = None,
    limit: int = 10,
    min_score: float = 0.5
) -> List[DocumentSearchResult]:
    """
    Semantic search across documents.

    Args:
        query: Search query text
        case_id: Filter to specific case (uses case collection)
        client_id: Filter to specific client
        limit: Max results to return
        min_score: Minimum similarity threshold

    Returns: List of DocumentSearchResult with score, metadata
    """

async def graph_enhanced_search(
    query: str,
    case_id: str = None,
    limit: int = 20,
    use_graph: bool = True,
    use_reranking: bool = True,
    semantic_weight: float = 0.7
) -> List[SearchResult]:
    """
    Full hybrid search: semantic + keyword + graph expansion.

    Returns: Ranked search results with context
    """
```

### Document Processing

```python
async def extract_text_from_file(file: UploadFile) -> str:
    """
    Extract text using Tika.
    Handles: PDF, DOCX, TXT, HTML, images (OCR)

    Returns: Extracted text string
    """

async def smart_extract(
    file: UploadFile,
    force_docling: bool = False
) -> Tuple[str, dict]:
    """
    Smart routing between Tika and Docling.

    Auto-routes to Docling for:
    - Images (better OCR)
    - PDFs with detected tables

    Returns: (extracted_text, metadata)
    """

def chunk_text(
    text: str,
    max_size: int = 2500,
    overlap: int = 500
) -> List[str]:
    """
    Split text into overlapping chunks.
    Respects sentence boundaries where possible.

    Returns: List of chunk strings
    """
```

### Case Management

```python
def load_cases_data() -> dict:
    """
    Load cases.json from disk.

    Returns: {"cases": [...], "last_updated": "..."}
    """

def save_cases_data(data: dict) -> None:
    """
    Save cases.json to disk.
    Automatically updates last_updated timestamp.
    """

def get_case(case_id: str) -> Optional[dict]:
    """Get single case by ID."""

def get_client(case_id: str, client_id: str) -> Optional[dict]:
    """Get client within a case."""
```

### Self-Reflection Loop

```python
async def self_reflect_response(
    query: str,
    initial_response: str,
    source_context: str,
    max_iterations: int = 2,
    confidence_threshold: float = 0.8,
) -> ReflectionResult:
    """
    Iterative self-critique loop: Generate → Critique → Refine → Verify.

    The critique step evaluates for:
    - UNSUPPORTED_CLAIM: Statements not backed by sources
    - LOGICAL_GAP: Missing reasoning steps
    - MISSING_CONTEXT: Important source info omitted
    - CONTRADICTION: Conflicts with sources or itself
    - HALLUCINATION: Fabricated facts or citations

    Args:
        query: Original user query
        initial_response: First LLM response to validate
        source_context: Document context for grounding
        max_iterations: Max refinement attempts (clamped 1-3)
        confidence_threshold: Stop when confidence >= this

    Returns: ReflectionResult with:
        - final_response: Refined response text
        - iterations: Number of critique/refine cycles
        - confidence: Final confidence score (0.0-1.0)
        - issues_fixed: List of issue types that were corrected
        - reflection_time_ms: Total time spent in reflection
    """
```

### External APIs

```python
async def search_case_law(
    query: str,
    jurisdiction: str = None,
    max_results: int = 10
) -> List[dict]:
    """
    Search CourtListener for case law.
    PII should be stripped before calling.

    Returns: List of case citations with excerpts
    """
```

### Memory Context (memory/context.py)

```python
def get_or_create_session(
    case_id: str,
    user_id: str,
    session_id: str = None
) -> str:
    """
    Get existing session or create new one.

    If session_id provided and valid, returns it.
    Otherwise creates new session with auto-generated title.

    Returns: session_id string
    """

def get_memory_context(
    case_id: str,
    session_id: str,
    query_embedding: List[float],
    max_tokens: int = 2000,
    include_recent: int = 5,
    include_semantic: int = 5
) -> Tuple[str, dict]:
    """
    Build memory context string for RAG prompts.

    Combines:
    - Recent messages from current session
    - Semantically similar past messages (from other sessions)

    Returns: (context_string, metadata_dict)

    metadata_dict: {
        "session_id": str,
        "recent_messages": int,
        "semantic_matches": int,
        "total_tokens": int
    }
    """

def store_exchange(
    session_id: str,
    user_message: str,
    assistant_response: str,
    user_embedding: List[float] = None,
    response_embedding: List[float] = None,
    documents_cited: List[dict] = None,
    entities_mentioned: List[str] = None
):
    """
    Store a Q&A exchange in memory.

    Stores both messages in SQLite, optionally in Qdrant if embeddings provided.
    Also tracks cited documents and mentioned entities for context.
    """
```

### Analysis Helpers (analysis/*.py)

```python
# analysis/classifier.py
async def classify_document(text: str) -> Tuple[str, float]:
    """
    Classify document type using LLM.

    Returns: (document_type, confidence)
    Types: contract, email, pleading, deposition, medical, financial, correspondence
    """

# analysis/summarizer.py
async def generate_summary(text: str, doc_type: str = None) -> str:
    """
    Generate concise summary using LLM.

    Returns: 2-3 sentence summary focused on legal relevance
    """

# analysis/fact_extractor.py
async def extract_key_facts(text: str) -> List[dict]:
    """
    Extract dates, amounts, parties using LLM.

    Returns: [{"type": "date", "value": "2024-01-15", "context": "filing deadline"}, ...]
    """

# analysis/hot_scorer.py
async def calculate_hot_score(text: str, facts: List[dict]) -> Tuple[int, dict]:
    """
    Calculate importance score 0-100.

    Factors: deadlines, money amounts, privileged content, key parties, urgency words

    Returns: (score, factors_dict)
    """

# analysis/privilege_detector.py
async def detect_privilege(text: str) -> Tuple[List[str], float, bool]:
    """
    Detect attorney-client or work product privilege indicators.

    Returns: (privilege_flags, confidence, needs_review)
    """

# analysis/fact_extractor.py - Timeline extraction
def extract_timeline_events(
    doc_id: str,
    content: str,
    doc_type: str,
    filename: str
) -> List[dict]:
    """
    Extract dated events from document content using LLM.

    Returns: List of event dicts with:
        - date: ISO format (YYYY-MM-DD) or approximate ("early March 2024")
        - date_precision: exact|month|quarter|approximate
        - description: What happened
        - event_type: meeting|communication|employment|legal|filing|medical|financial
        - claimed_by: Who made this claim (document author)
        - about_whom: Who is this event about
        - source_quote: Original text mentioning date (max 200 chars)
        - confidence: 0.0-1.0
    """

# analysis/inconsistency_detector.py
def detect_inconsistencies(
    case_id: str,
    new_document_id: str = None
) -> List[dict]:
    """
    Compare events across documents to find conflicts.

    Detection strategies:
    1. Date mismatch: Same event described with different dates (>3 day variance)
    2. Timeline impossible: Event B before Event A when B depends on A
    3. Contradictory claims: Conflicting statements about same event

    Args:
        case_id: Case to check
        new_document_id: If provided, only check conflicts involving this doc

    Returns: List of inconsistency dicts with:
        - inconsistency_id: Unique ID
        - event_id_1, event_id_2: The conflicting events
        - conflict_type: date_mismatch|timeline_impossible|contradictory_claim
        - description: Human-readable explanation
        - severity: low|medium|high|critical
    """
```

---

## 3. Data Models

### Core Models

```python
class Case(BaseModel):
    case_id: str
    case_name: str
    case_number: Optional[str]
    description: Optional[str]
    clients: List[Client] = []
    research: List[LegalResearchResult] = []
    created_at: datetime
    updated_at: datetime

class Client(BaseModel):
    client_id: str
    name: str
    notes: Optional[str]
    created_at: datetime

class Document(BaseModel):
    doc_id: str
    filename: str
    case_id: str
    client_id: str
    file_path: Optional[str]
    content_type: str
    chunks_count: int
    uploaded_at: datetime

class LegalResearchResult(BaseModel):
    research_id: str
    case_id: str
    query: str
    sanitized_query: str
    cases_found: List[dict]
    summary: str
    created_at: datetime
```

### Request/Response Models

```python
# Document Upload
class UploadDocumentResponse(BaseModel):
    document_id: str
    filename: str
    chunks_count: int
    status: str
    entities_queued: bool

# Search
class SearchRequest(BaseModel):
    query: str
    case_id: Optional[str]
    limit: int = 10
    min_score: float = 0.5

class SearchResult(BaseModel):
    doc_id: str
    chunk_id: str
    filename: str
    content: str
    score: float
    metadata: dict

# RAG Query (with memory + self-reflection support)
class RAGQueryRequest(BaseModel):
    query: str
    case_id: Optional[str]
    client_id: Optional[str]
    max_context_chunks: int = 5
    include_sources: bool = True
    use_memory: bool = False       # Enable conversational memory
    user_id: Optional[str] = None  # Required for memory
    session_id: Optional[str] = None  # Resume existing session
    use_reflection: bool = False   # Enable self-critique loop
    reflection_max_iterations: int = 2  # Max refinement iterations (1-3)

class RAGQueryResponse(BaseModel):
    response: str
    sources: List[dict]
    query_anonymized: bool
    tokens_used: int
    memory_metadata: Optional[dict] = None  # Memory context stats
    reflection_metadata: Optional[dict] = None  # Self-reflection stats

# Analysis
class AnalyzeAllRequest(BaseModel):
    case_id: str
    query: str
    mode: Optional[str]  # "inventory", "timeline", "compare", "aggregate"
    max_documents: int = 50
    min_relevance: float = 0.25

class AnalyzeAllResponse(BaseModel):
    analysis: str
    mode_used: str
    documents_analyzed: int
    document_list: List[dict]
    memory_metadata: Optional[dict] = None  # Memory context stats (if enabled)

# Analysis Pipeline Models
class AnalysisRequest(BaseModel):
    document_id: str
    case_id: str
    priority: int = 5      # 1-10, higher = more urgent
    force: bool = False    # Re-analyze even if already done

class AnalysisResult(BaseModel):
    document_id: str
    case_id: str
    document_type: Optional[str]       # e.g., "contract", "email", "pleading"
    type_confidence: Optional[float]   # 0.0-1.0
    summary: Optional[str]             # LLM-generated summary
    key_facts: List[Dict]              # [{type: "date", value: "2024-01-15"}, ...]
    hot_score: Optional[int]           # 0-100 importance score
    hot_factors: Dict                  # {"has_deadline": true, "mentions_money": true}
    privilege_flags: List[str]         # ["attorney_client", "work_product"]
    privilege_confidence: Optional[float]
    needs_review: bool                 # True if human review needed
    analyzed_at: Optional[str]

# Memory Models
class SessionResponse(BaseModel):
    session_id: str
    case_id: str
    user_id: str
    title: str
    created_at: str
    updated_at: str
    message_count: int
    token_count: int
    summary: Optional[str]

class MessageResponse(BaseModel):
    message_id: str
    session_id: str
    role: str                           # "user" or "assistant"
    content: str
    timestamp: str
    tokens: int
    documents_cited: List[Dict] = []    # [{document_id, filename}, ...]
    entities_mentioned: List[str] = []  # ["John Smith", "Acme Corp"]

# Timeline Models
class TimelineEvent(BaseModel):
    event_id: str
    case_id: str
    document_id: str
    event_date: Optional[str]           # YYYY-MM-DD or approximate
    event_date_precision: str           # exact|month|quarter|approximate
    event_description: str
    event_type: str                     # meeting|communication|employment|legal|etc.
    claimed_by: Optional[str]           # Document author
    about_whom: Optional[str]           # Subject of event
    source_quote: Optional[str]         # Original text (max 200 chars)
    confidence: float
    has_conflict: bool = False          # True if involved in inconsistency

class TimelineInconsistency(BaseModel):
    inconsistency_id: str
    case_id: str
    event_id_1: str
    event_id_2: str
    event_1: Optional[TimelineEvent]    # Populated in API responses
    event_2: Optional[TimelineEvent]
    conflict_type: str                  # date_mismatch|timeline_impossible|contradictory_claim
    description: str
    severity: str                       # low|medium|high|critical
    resolved: bool = False
    resolution_notes: Optional[str]
    created_at: str
```

---

## 4. Key Endpoints

### Health & Status

| Endpoint | Method | Purpose |
|----------|--------|---------|
| `/health` | GET | Basic health check |
| `/health/extractors` | GET | Tika/Docling status |
| `/metrics` | GET | Prometheus metrics |

### System & Monitoring

| Endpoint | Method | Purpose |
|----------|--------|---------|
| `/system/gpu-scheduler` | GET | GPU scheduler status (mode, queue) |
| `/system/jobs` | GET | Active background jobs |
| `/system/agent-jobs` | GET | Agent job statuses |
| `/system/agent-jobs/{id}` | DELETE | Cancel agent job |
| `/system/agent-queue-stats` | GET | Agent queue statistics |
| `/system/graph-failures` | GET | Unresolved Neo4j failures |
| `/system/reconcile-graph` | POST | Trigger graph reconciliation |
| `/analysis/queue/status` | GET | Analysis queue status |
| `/analysis/queue/paused` | GET | Check if queue is paused |
| `/analysis/queue/pause` | POST | Pause analysis queue |
| `/analysis/queue/resume` | POST | Resume analysis queue |
| `/analysis/queue/processing` | GET | Currently processing docs |

### Cases & Clients

| Endpoint | Method | Purpose |
|----------|--------|---------|
| `/cases` | GET | List all cases |
| `/cases` | POST | Create case |
| `/cases/{id}` | GET | Get case details |
| `/cases/{id}` | DELETE | Delete case |
| `/cases/{id}/clients` | POST | Add client to case |
| `/cases/{id}/clients/{cid}` | DELETE | Remove client |

### Documents

| Endpoint | Method | Purpose |
|----------|--------|---------|
| `/upload-document` | POST | Upload document |
| `/documents/{id}` | GET | Get document metadata |
| `/documents/{id}` | DELETE | Delete document |
| `/documents/{id}/preview` | GET | Preview document content |
| `/documents/{id}/file` | GET | Download original file |
| `/clients/{cid}/documents` | GET | List client's documents |

### Search & RAG

| Endpoint | Method | Purpose |
|----------|--------|---------|
| `/search-documents` | GET | Semantic search |
| `/rag-query` | POST | RAG question answering |
| `/analyze-all` | POST | Bulk document analysis |

### PII

| Endpoint | Method | Purpose |
|----------|--------|---------|
| `/detect-pii` | POST | Detect PII in text |
| `/anonymize` | POST | Anonymize text |

### Legal Research

| Endpoint | Method | Purpose |
|----------|--------|---------|
| `/cases/{id}/smart-search/preview` | POST | Generate search query |
| `/cases/{id}/smart-search/execute` | POST | Execute and save research |
| `/cases/{id}/research` | GET | List saved research |

### Graph (Knowledge Graph)

| Endpoint | Method | Purpose |
|----------|--------|---------|
| `/graph/entities` | GET | List entities |
| `/graph/entity/{id}` | GET | Entity details |
| `/graph/entity/{id}/relationships` | GET | Entity relationships |
| `/graph/stats` | GET | Graph statistics |
| `/graph/communities/case/{case_id}` | GET | Get entity communities for a case (UI Graph Explorer) |
| `/graph/related-entities` | GET | Get entities within N hops of a source entity |
| `/graph/path-finding` | POST | Find paths between two entities |
| `/graph/traverse` | GET | Traverse graph from starting node |

### Analysis (Auto-Analysis Pipeline)

| Endpoint | Method | Purpose |
|----------|--------|---------|
| `/analysis/queue` | POST | Queue document for analysis |
| `/analysis/result/{doc_id}` | GET | Get analysis results |
| `/analysis/queue/status` | GET | Queue and broker status |
| `/analysis/hot-documents/{case_id}` | GET | Documents sorted by importance |
| `/analysis/privilege-review/{case_id}` | GET | Docs needing privilege review |
| `/analysis/batch-queue` | POST | Queue multiple documents |

### Memory (Conversational Memory)

| Endpoint | Method | Purpose |
|----------|--------|---------|
| `/memory/sessions` | POST | Create conversation session |
| `/memory/sessions` | GET | List sessions for case |
| `/memory/sessions/{id}` | GET | Get session details |
| `/memory/sessions/{id}` | DELETE | Delete session + messages |
| `/memory/sessions/{id}/title` | PATCH | Update session title |
| `/memory/sessions/{id}/messages` | POST | Add message to session |
| `/memory/sessions/{id}/messages` | GET | Get session messages |
| `/memory/sessions/{id}/recent` | GET | Get N recent messages |
| `/memory/sessions/{id}/messages/{mid}/feedback` | POST | Record message feedback |
| `/memory/sessions/{id}/context` | GET | Get entities/docs discussed |
| `/memory/search` | POST | Text search past conversations |
| `/memory/semantic-search` | POST | Vector search past conversations |
| `/memory/stats/{case_id}` | GET | Memory stats for case |

### Timeline (Event Extraction & Inconsistency Detection)

| Endpoint | Method | Purpose |
|----------|--------|---------|
| `/timeline/{case_id}/events` | GET | Get all extracted events for a case |
| `/timeline/{case_id}/inconsistencies` | GET | Get detected timeline inconsistencies |
| `/timeline/{case_id}/inconsistencies/{id}/resolve` | POST | Mark inconsistency as resolved |
| `/timeline/{case_id}/trigger-detection` | POST | Re-run inconsistency detection |

#### Query Parameters for `/timeline/{case_id}/events`:
- `start_date` (str): Filter events after this date (YYYY-MM-DD)
- `end_date` (str): Filter events before this date
- `event_type` (str): Filter by type (meeting, communication, employment, legal, etc.)
- `person` (str): Filter events about specific person

### Agent Analysis (Case Analysis Tab)

Specialized agent endpoints for case analysis. Used by the Case Analysis tab.

| Endpoint | Method | Purpose |
|----------|--------|---------|
| `/agents/{case_id}/riley/deadlines` | POST | Get legal deadlines with countdown |
| `/agents/{case_id}/david/weaknesses` | POST | Get case weaknesses by severity |
| `/agents/{case_id}/elena/elements` | POST | Get case elements by claim type |
| `/agents/{case_id}/judge-chen/bias` | POST | Analyze judge for potential biases |

**Query Parameters for Judge Chen:**
- `judge_name` (str): Name of the judge to analyze

**Response Formats:**
```json
// RILEY Deadlines
{
  "deadlines": [
    {
      "name": "Answer to Complaint",
      "due_date": "2026-02-15",
      "rule_citation": "FRCP 12(a)(1)",
      "description": "Response deadline from service date"
    }
  ]
}

// DAVID Weaknesses
{
  "weaknesses": [
    {
      "name": "Missing witness corroboration",
      "severity": "serious",  // fatal, serious, manageable, minor
      "description": "Key witness statement lacks independent verification",
      "opposing_attack": "Defense will challenge credibility",
      "mitigation": "Seek additional supporting documentation"
    }
  ]
}

// ELENA Elements
{
  "elements_by_claim": {
    "Negligence": [
      {
        "name": "Duty of Care",
        "strength": "strong",  // strong, moderate, weak, harmful
        "evidence": "Employment contract establishes duty"
      }
    ]
  }
}

// JUDGE CHEN Bias
{
  "indicators": [
    {
      "type": "Pro-defense rulings",
      "severity": "notable",  // concerning, notable, minor
      "description": "60% of summary judgment motions granted for defense",
      "recommendation": "Prepare stronger factual record"
    }
  ]
}
```

### Agent Chat (Multi-Agent Collaboration)

| Endpoint | Method | Purpose |
|----------|--------|---------|
| `/agents/chat` | POST | Send message to agents, get responses |
| `/agents/chat/{case_id}` | GET | Retrieve conversation history |
| `/agents/chat/{case_id}` | DELETE | Clear conversation history |
| `/agents/simulate` | POST | Simulate multi-agent discussion |
| `/agents/status` | GET | Get status of all background agents |
| `/agents/status/{case_id}/{agent}` | GET | Get specific agent status |
| `/agents/run/{agent}/{case_id}` | POST | Trigger agent on case |

**Available Agents:**
- `riley` - Paralegal (deadlines, procedures)
- `david` - Devil's Advocate (weaknesses, counterarguments)
- `marcus` - Investigator (contradictions, red flags)
- `elena` - Damages Expert (elements, damages)
- `sophia` - Medical Expert (injuries, causation)
- `bench` - Bench perspective (judicial analysis)
- `opposing` / `harrison` - Opposing counsel (adversarial)

**Chat Request Body:**
```json
{
  "message": "@riley What are the key deadlines?",
  "case_id": "case_123",
  "active_agents": ["riley", "marcus", "elena"],
  "sequential_mode": true  // One agent at a time
}
```

### Agent Findings (Background Analysis Results)

Background agents (Scrutinizer, Extractor, etc.) store their findings in the `agent_findings` table. These are surfaced in RAG responses automatically.

| Endpoint | Method | Purpose |
|----------|--------|---------|
| `/cases/{case_id}/findings` | GET | Get all agent findings for a case |
| `/cases/{case_id}/findings/stats` | GET | Get finding counts by severity |

**Query Parameters for `/findings`:**
- `agent_name` - Filter by agent (e.g., "Scrutinizer")
- `finding_type` - Filter by type ("issue" or "finding")
- `severity` - Filter by severity ("CRITICAL", "MAJOR", "MINOR")
- `limit` - Max results (default 50)

**Response Example:**
```json
{
  "case_id": "fd6c85f1",
  "findings": [
    {
      "finding_id": "8e1403a3-ee4",
      "agent_name": "Scrutinizer",
      "finding_type": "issue",
      "severity": "CRITICAL",
      "issue_type": "contradiction",
      "description": "Witness A stated 2pm Jan 5, Witness B stated 10am Jan 4",
      "source_doc_ids": ["doc_001", "doc_002"],
      "excerpts": ["Meeting was at 2pm", "I remember 10am"],
      "suggested_action": "Cross-examine both witnesses",
      "created_at": "2026-01-15 08:25:26"
    }
  ],
  "count": 1
}
```

**Stats Response:**
```json
{
  "case_id": "fd6c85f1",
  "stats": {
    "total": 5,
    "critical": 1,
    "major": 2,
    "minor": 2,
    "agents_run": 1
  }
}
```

**RAG Context Integration:**
Critical and major findings are automatically injected at the top of RAG context via `get_memory_context()`:
```
IMPORTANT - Background analysis found these issues:
  ⚠️ CRITICAL: Witness timeline contradiction...
    → Action: Cross-examine witnesses
  ⚡ MAJOR: Invoice/contract amount mismatch...
```

---

## 5. Code Patterns

### Adding an Endpoint

```python
from fastapi import APIRouter, HTTPException
from pydantic import BaseModel

class MyRequest(BaseModel):
    text: str
    option: bool = False

class MyResponse(BaseModel):
    result: str
    count: int

@app.post("/my-endpoint", response_model=MyResponse)
async def my_endpoint(request: MyRequest):
    """
    Description of what this endpoint does.
    """
    try:
        # 1. Validate input
        if not request.text:
            raise HTTPException(status_code=400, detail="Text required")

        # 2. Process
        result = await some_helper_function(request.text)

        # 3. Return response
        return MyResponse(result=result, count=len(result))

    except Exception as e:
        logger.error(f"Error in my_endpoint: {e}")
        raise HTTPException(status_code=500, detail=str(e))
```

### Using PII Pipeline

```python
@app.post("/my-safe-endpoint")
async def my_safe_endpoint(request: QueryRequest):
    # 1. Detect and anonymize PII
    anonymized_text, token_map = await anonymize_text(request.text)

    # 2. Process with anonymized text
    result = await query_ollama(anonymized_text)

    # 3. Restore PII in response
    final_result = restore_pii(result, token_map)

    return {"result": final_result}
```

### Querying Qdrant

```python
# Basic search
results = qdrant_client.search(
    collection_name=COLLECTION_NAME,
    query_vector=await get_embedding(query),
    limit=10,
    query_filter=models.Filter(
        must=[
            models.FieldCondition(
                key="case_id",
                match=models.MatchValue(value=case_id)
            )
        ]
    )
)

# Access results
for hit in results:
    doc_id = hit.payload["doc_id"]
    content = hit.payload["content"]
    score = hit.score
```

### Querying Neo4j

```python
from graph_utils import get_neo4j_session

async def get_related_entities(entity_id: str):
    with get_neo4j_session() as session:
        result = session.run("""
            MATCH (e:Entity {entity_id: $id})-[r]-(related)
            RETURN type(r) as relationship, related
            LIMIT 50
        """, id=entity_id)

        return [dict(record) for record in result]
```

### Background Task Pattern

```python
# Define worker
async def my_background_worker():
    while True:
        task = await my_queue.get()
        try:
            await process_task(task)
        except Exception as e:
            logger.error(f"Worker error: {e}")
        finally:
            my_queue.task_done()

# Start at app startup
@app.on_event("startup")
async def startup():
    asyncio.create_task(my_background_worker())
```

### Lazy Model Loading

```python
my_model = None

def load_my_model():
    global my_model
    if my_model is None:
        logger.info("Loading my_model...")
        my_model = SomeModel.from_pretrained("model-name")
    return my_model

def unload_my_model():
    global my_model
    if my_model is not None:
        my_model = None
        gc.collect()
        torch.cuda.empty_cache()
        logger.info("Unloaded my_model")
```

---

## 6. Adding New Features

### Checklist

1. **Define Models**
   - Request model (what the endpoint receives)
   - Response model (what it returns)
   - Add to appropriate section in main.py

2. **Write Helper Functions**
   - Keep business logic in helpers, not endpoints
   - Use existing patterns (PII, embeddings, etc.)

3. **Create Endpoint**
   - Follow existing patterns
   - Add proper error handling
   - Document with docstring

4. **Update UI (if needed)**
   - HTML is embedded in main.py (~lines 200-1200)
   - JavaScript functions at end of HTML section

5. **Add Tests**
   - Test happy path
   - Test error cases
   - Test edge cases

6. **Update Documentation**
   - Add to this API_REFERENCE.md
   - Update CHANGELOG.md

### Example: Adding a New Analysis Mode

```python
# 1. Add to ANALYSIS_PROMPTS dict
ANALYSIS_PROMPTS["new_mode"] = """
You are analyzing documents for [purpose].
Instructions...
"""

# 2. Update detect_analysis_mode()
def detect_analysis_mode(query: str) -> str:
    query_lower = query.lower()
    if any(kw in query_lower for kw in ["new_keyword", "other_keyword"]):
        return "new_mode"
    # ... existing logic

# 3. Update AnalyzeAllRequest model if needed
class AnalyzeAllRequest(BaseModel):
    mode: Optional[str]  # Add "new_mode" to allowed values

# 4. Test via API
curl -X POST http://localhost:8000/analyze-all \
  -H "Content-Type: application/json" \
  -d '{"case_id": "xxx", "query": "new_keyword query", "mode": "new_mode"}'
```

---

# External Service APIs

> **Credentials:** All API keys stored in `/docker/.secrets/` - see table below.
>
> **Quick Commands:** See `docs/reference/CLAUDE_CHEATSHEET.md` for copy-paste commands with learned correct syntax.

### API Keys Location

| Service | Secret File | Purpose |
|---------|-------------|---------|
| Authentik | `/docker/.secrets/authentik_token` | SSO admin API |
| Grafana | `/docker/.secrets/grafana_api_key` | Dashboard API |
| Unifi UDMSE | `/docker/.secrets/unifi_api_key` | Network controller API |

---

## 7. Authentik SSO API

### Overview

Authentik provides SSO for all SynSeekr services via forward auth through Traefik.

| Property | Value |
|----------|-------|
| **Base URL** | `https://192.168.1.50:9443/api/v3` (external) or `http://localhost:9000/api/v3` (internal) |
| **Auth Header** | `Authorization: Bearer <token>` |
| **Token Location** | `/docker/.secrets/authentik_token` |
| **Docs** | https://docs.goauthentik.io/developer-docs/api/ |

### Get Token from Database (if file unreadable)

```bash
docker exec authentik-postgresql psql -U authentik -d authentik \
  -c "SELECT key FROM authentik_core_token WHERE identifier NOT LIKE '%outpost%' AND intent = 'api' LIMIT 1;" -t | tr -d ' '
```

### Key UUIDs (for SSO app setup)

| UUID | Purpose |
|------|---------|
| `81ee08b0-c5bb-4766-a78b-4832f0e0d389` | Implicit consent authorization flow |
| `3abde679-f1f7-40d5-9173-65a6ab03e9ad` | Invalidation flow |
| `29c35369-03da-4f4d-8fcf-5b935820aa8a` | Embedded Outpost ID |
| `aadc578d-8994-472d-bfb6-ffd17bca8827` | Alternative invalidation flow |
| `f88fc9db-d389-4762-8661-b80a23d5c865` | Brand ID |

### Common Endpoints

```bash
# Setup
TOKEN=$(cat /docker/.secrets/authentik_token)
AUTH_URL="https://192.168.1.50:9443/api/v3"

# List all applications
curl -sk "$AUTH_URL/core/applications/" -H "Authorization: Bearer $TOKEN" | jq '.results[] | {name, slug}'

# List all providers
curl -sk "$AUTH_URL/providers/all/" -H "Authorization: Bearer $TOKEN"
curl -sk "$AUTH_URL/providers/proxy/" -H "Authorization: Bearer $TOKEN"

# List users
curl -sk "$AUTH_URL/core/users/" -H "Authorization: Bearer $TOKEN"

# List groups
curl -sk "$AUTH_URL/core/groups/" -H "Authorization: Bearer $TOKEN"

# Get outposts
curl -sk "$AUTH_URL/outposts/instances/" -H "Authorization: Bearer $TOKEN"

# Get flows
curl -sk "$AUTH_URL/flows/instances/" -H "Authorization: Bearer $TOKEN"

# Get brands (theming)
curl -sk "$AUTH_URL/core/brands/" -H "Authorization: Bearer $TOKEN"
```

### Create Proxy Provider

```bash
curl -sk -X POST "$AUTH_URL/providers/proxy/" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "name": "AppName Proxy Provider",
    "authorization_flow": "81ee08b0-c5bb-4766-a78b-4832f0e0d389",
    "invalidation_flow": "3abde679-f1f7-40d5-9173-65a6ab03e9ad",
    "external_host": "https://appname.synseekr.localdomain",
    "internal_host": "http://container:port",
    "mode": "forward_domain",
    "cookie_domain": "synseekr.localdomain"
  }'
# Returns: {"pk": <provider_id>, ...} - save pk for next step
```

### Create Application

```bash
curl -sk -X POST "$AUTH_URL/core/applications/" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "name": "App Display Name",
    "slug": "appslug",
    "provider": <provider_pk>,
    "meta_launch_url": "https://appname.synseekr.localdomain/",
    "meta_description": "Description of the app",
    "open_in_new_tab": true,
    "group": "Core",
    "policy_engine_mode": "any"
  }'
```

### Add Provider to Outpost

```bash
# Get current providers list first
curl -sk "$AUTH_URL/outposts/instances/29c35369-03da-4f4d-8fcf-5b935820aa8a/" \
  -H "Authorization: Bearer $TOKEN" | jq '.providers'

# Add new provider (include all existing + new)
curl -sk -X PATCH "$AUTH_URL/outposts/instances/29c35369-03da-4f4d-8fcf-5b935820aa8a/" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"providers": [1, 2, 3, 4, 9, <new_pk>]}'
```

### Set Application Icon

```bash
# Via URL
curl -sk -X POST "$AUTH_URL/core/applications/<slug>/set_icon_url/" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"url": "https://example.com/icon.png"}'

# Via file upload
curl -sk -X POST "$AUTH_URL/core/applications/<slug>/set_icon/" \
  -H "Authorization: Bearer $TOKEN" \
  -F "file=@/path/to/icon.svg"
```

### Create User

```bash
curl -sk -X POST "$AUTH_URL/core/users/" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "username": "newuser",
    "name": "New User",
    "email": "newuser@local",
    "groups": ["<group_uuid>"],
    "is_active": true
  }'
```

### Python Script: Full SSO App Creation

```python
#!/usr/bin/env python3
"""Create a complete SSO application in Authentik."""
import requests

# Load token
with open("/docker/.secrets/authentik_token") as f:
    TOKEN = f.read().strip()

BASE = "http://localhost:9000/api/v3"
HEADERS = {"Authorization": f"Bearer {TOKEN}", "Content-Type": "application/json"}

# Flow UUIDs
AUTH_FLOW = "81ee08b0-c5bb-4766-a78b-4832f0e0d389"
INVAL_FLOW = "3abde679-f1f7-40d5-9173-65a6ab03e9ad"
OUTPOST = "29c35369-03da-4f4d-8fcf-5b935820aa8a"

# App config
APP_NAME = "NewApp"
APP_SLUG = "newapp"
EXTERNAL_HOST = "https://newapp.synseekr.localdomain"
INTERNAL_HOST = "http://newapp:8080"

# 1. Create Proxy Provider
provider = requests.post(f"{BASE}/providers/proxy/", headers=HEADERS, json={
    "name": f"{APP_NAME} Proxy Provider",
    "authorization_flow": AUTH_FLOW,
    "invalidation_flow": INVAL_FLOW,
    "external_host": EXTERNAL_HOST,
    "internal_host": INTERNAL_HOST,
    "mode": "forward_domain",
    "cookie_domain": "synseekr.localdomain"
}).json()
provider_id = provider["pk"]
print(f"Created provider: {provider_id}")

# 2. Create Application
app = requests.post(f"{BASE}/core/applications/", headers=HEADERS, json={
    "name": APP_NAME,
    "slug": APP_SLUG,
    "provider": provider_id,
    "meta_launch_url": f"{EXTERNAL_HOST}/",
    "meta_description": f"{APP_NAME} application",
    "open_in_new_tab": True,
    "group": "Apps",
    "policy_engine_mode": "any"
}).json()
print(f"Created application: {app['slug']}")

# 3. Add Provider to Outpost
outpost = requests.get(f"{BASE}/outposts/instances/{OUTPOST}/", headers=HEADERS).json()
providers = outpost["providers"] + [provider_id]
requests.patch(f"{BASE}/outposts/instances/{OUTPOST}/", headers=HEADERS,
               json={"providers": providers})
print(f"Added provider to outpost. Total providers: {len(providers)}")

print(f"\nDone! Now add Traefik router in /docker/traefik/dynamic.yml")
```

---

## 8. Unifi Network API

### Overview

Unifi UDMSE manages DNS records for `*.synseekr.localdomain` via the Integration API v1.

| Property | Value |
|----------|-------|
| **Controller** | https://192.168.1.1 (Alex Home SE) |
| **API Base** | `https://192.168.1.1/proxy/network/integration/v1` |
| **Auth Header** | `X-API-KEY: <key>` |
| **Key Location** | `/docker/.secrets/unifi_api_key` |
| **Site ID** | `88f7af54-98f8-306a-a1c7-c9349722b1f6` (default) |

### CORRECT API Path (Integration API v1)

```bash
# Setup
UNIFI_KEY=$(cat /docker/.secrets/unifi_api_key)
SITE_ID="88f7af54-98f8-306a-a1c7-c9349722b1f6"
BASE="https://192.168.1.1/proxy/network/integration/v1"

# Test connection - list sites
curl -sk "$BASE/sites" \
  -H "X-API-KEY: $UNIFI_KEY" \
  -H "Accept: application/json"

# List DNS policies (A records)
curl -sk "$BASE/sites/${SITE_ID}/dns/policies" \
  -H "X-API-KEY: $UNIFI_KEY" \
  -H "Accept: application/json"
```

### WRONG API Paths (DO NOT USE)

These paths return errors - documented here to avoid wasting tokens:

| Wrong Path | Error |
|------------|-------|
| `/proxy/network/api/s/default/rest/dnsrecord` | InvalidObject |
| `/proxy/network/api/s/default/list/dnsrecord` | InvalidObject |
| `/proxy/network/v2/api/site/default/dns/records` | 404 Not Found |

### Create DNS A Record

```bash
curl -sk -X POST "$BASE/sites/${SITE_ID}/dns/policies" \
  -H "X-API-KEY: $UNIFI_KEY" \
  -H "Accept: application/json" \
  -H "Content-Type: application/json" \
  -d '{
    "type": "A_RECORD",
    "enabled": true,
    "domain": "newapp.synseekr.localdomain",
    "ipv4Address": "192.168.1.50",
    "ttlSeconds": 14400
  }'
# Returns: {"id": "<uuid>", ...} - save id for deletion
```

### Delete DNS Record

```bash
curl -sk -X DELETE "$BASE/sites/${SITE_ID}/dns/policies/<dnsPolicyId>" \
  -H "X-API-KEY: $UNIFI_KEY"
```

### Current DNS Records (as of 2026-01-11)

| Domain | IP | Record ID |
|--------|-----|-----------|
| app.synseekr.localdomain | 192.168.1.50 | `9ea2cf67-c2b3-46db-b8ad-9d4e73aa148b` |
| auth.synseekr.localdomain | 192.168.1.50 | `060c9b4d-0184-4967-9efd-3fb845bf92f5` |
| chat.synseekr.localdomain | 192.168.1.50 | `5cbfff7c-ce8b-4bb0-99ad-e7e541b71366` |
| grafana.synseekr.localdomain | 192.168.1.50 | `5ead96cf-0682-4225-8cb9-3223f7684a40` |
| neo4j.synseekr.localdomain | 192.168.1.50 | `a9aa1690-bbca-43b9-ad9e-5af839a7f2f2` |
| qdrant.synseekr.localdomain | 192.168.1.50 | *(added 2026-01-12)* |

---

## 9. Grafana API

### Overview

Grafana provides observability dashboards for SynSeekr.

| Property | Value |
|----------|-------|
| **Base URL** | `http://localhost:3000/api` |
| **Auth Header** | `Authorization: Bearer <key>` |
| **Key Location** | `/docker/.secrets/grafana_api_key` |

### Common Endpoints

```bash
# Setup
GRAFANA_KEY=$(cat /docker/.secrets/grafana_api_key)

# List dashboards
curl -s "http://localhost:3000/api/search?type=dash-db" \
  -H "Authorization: Bearer $GRAFANA_KEY" | jq

# Get specific dashboard
curl -s "http://localhost:3000/api/dashboards/uid/<dashboard_uid>" \
  -H "Authorization: Bearer $GRAFANA_KEY"

# List datasources
curl -s "http://localhost:3000/api/datasources" \
  -H "Authorization: Bearer $GRAFANA_KEY"

# Health check
curl -s "http://localhost:3000/api/health"

# Current user
curl -s "http://localhost:3000/api/user" \
  -H "Authorization: Bearer $GRAFANA_KEY"

# List folders
curl -s "http://localhost:3000/api/folders" \
  -H "Authorization: Bearer $GRAFANA_KEY"
```

### Create Dashboard

```bash
curl -s -X POST "http://localhost:3000/api/dashboards/db" \
  -H "Authorization: Bearer $GRAFANA_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "dashboard": {
      "title": "New Dashboard",
      "panels": [],
      "schemaVersion": 30
    },
    "folderId": 0,
    "overwrite": false
  }'
```

---

## Related Documentation

- **System architecture:** `ARCHITECTURE.md`
- **Data workflows:** `WORKFLOWS.md`
- **Troubleshooting:** `TROUBLESHOOTING.md`
- **SSO setup guide:** `SSO_GUIDE.md`
- **Quick commands:** `../reference/CLAUDE_CHEATSHEET.md`

---

*[2026-01-12 17:45 MST] Added External Service APIs section (Authentik, Unifi, Grafana)*
*[2026-01-12] Added self-reflection loop (use_reflection parameter in RAG queries)*
